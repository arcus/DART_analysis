---
title: "Hypothesis Testing"
author: "Rose Hartman"
date: '2023-06-15'
output: github_document
---

```{r setup}
# load custom functions for plotting
library(ggplot2)
theme_set(theme_classic())
source(here::here("src", "scripts", "functions_plotting.R"))

# if the figures directory doesn't exist, create it
dir.create(here::here("reports"), showWarnings = FALSE)
dir.create(here::here("reports", "figures"), showWarnings = FALSE)

knitr::opts_chunk$set(
  echo = TRUE
)
```

`r if(!knitr::opts_chunk$get()$echo) ">Note that code chunks are not printed in this report in order to keep the output tidy. To see all of the code to generate these results, open the .Rmd file."`


This report runs the hypothesis tests as specified in [our preregistration on OSF](https://osf.io/zmnr6/?view_only=2d26a411c57d49aca1754b8920e57a71). 
The text descriptions preceeding the analyses in this report (except for this opening text) are copied directly from the preregistration.

Note that all of our [surveys are available on GH](https://github.com/arcus/education_dart_recruitment_comms/tree/main/surveys), including the exact wording of questions and options.

```{r load_data, message=FALSE}
waves <- readr::read_csv(here::here("participant_waves.csv"), show_col_types = FALSE)

nih_pre <- readRDS(here::here("data", "deidentified", "nih_pre.rds")) |> 
  dplyr::left_join(waves, by = "record_id") |> 
  dplyr::filter(wave == 1) |> 
  dplyr::mutate(event = "pre")
nih_post <- readRDS(here::here("data", "deidentified", "nih_post.rds")) |> 
  dplyr::left_join(waves, by = "record_id") |> 
  dplyr::filter(wave == 1)|> 
  dplyr::mutate(event = "post")
nih <- dplyr::bind_rows(nih_post, nih_pre)

exit <- readRDS(here::here("data", "deidentified", "exit_survey.rds")) |> 
  dplyr::left_join(waves, by = "record_id") |> 
  dplyr::filter(wave == 1)

# get pathway assignments
pathways <- readr::read_csv(here::here("participant_pathways.csv"), show_col_types = FALSE)
```

### Variables

> ability_pre and ability_post are the mean of participants' ratings (assessed pre- or post-program, respectively) on the following items from survey2: all items under the instructions "please rate your ability to complete the following tasks". 

> openscience_pre and openscience_post are the mean of participants' ratings (assessed pre- or post-program, respectively) on the following items from survey2: all items under the heading "please rate your level of agreement with the following statements"

```{r calculate_mean_scores}
nih <- nih |> 
  # make factors numeric, so we can calculate means
  dplyr::mutate(dplyr::across(tidyselect::where(is.factor), as.numeric)) |> 
  dplyr::rowwise() |> 
  dplyr::mutate(ability = mean(dplyr::c_across(findable:atomize), na.rm = TRUE),
                openscience = mean(dplyr::c_across(data_storage_1:code_efficient), na.rm = TRUE))
```

> Measurements from the exit survey will be combined into an engagement index capturing learners' level of engagement in the program. The two items "How many of your assigned learning modules did you complete?" and "How often did you engage with other learners in the program?" will be averaged to create the engagement index.

```{r calculate_engagement}
engagement <- exit |> 
  # make factors numeric, so we can calculate means
  dplyr::mutate(dplyr::across(tidyselect::where(is.factor), as.numeric)) |> 
  dplyr::rowwise() |> 
  dplyr::mutate(engagement = mean(c(completed_modules, interaction_how_often), na.rm = TRUE)) |> 
  # only keep record_id and the engagement index
  dplyr::select(record_id, engagement, completed_modules, interaction_how_often)
```

```{r data_for_testing}
# combine engagement with nih data, drop variables we don't need, get it in wide format for easier comparison
data <- nih |> 
  dplyr::select(record_id, event, ability, openscience) |> 
  tidyr::pivot_longer(cols = c(ability, openscience)) |> 
  tidyr::pivot_wider(id_cols = record_id, names_from = c(name, event), values_from = value) |> 
  dplyr::left_join(engagement, by = "record_id") |> 
  dplyr::left_join(pathways, by = "record_id") |> 
  # calculate change scores
  dplyr::mutate(ability_change = ability_post - ability_pre,
                openscience_change = openscience_post - openscience_pre) |> 
  dplyr::select(record_id, pathway, engagement, completed_modules, interaction_how_often, ends_with("change"), ends_with("pre"), ends_with("post")) |> 
  # only keep cases that have data for all three key variables
  dplyr::filter(!is.na(engagement) & !is.na(ability_change) & !is.na(openscience_change))

# save this file in case we want it handy for other notebooks
saveRDS(data, file = here::here("data", "interim", "data_for_hyp_testing_w1.rds"))
```

## Hypotheses

> 1. Learners' self-ratings of their own ability to perform data science tasks will increase over the course of the program (pre to post).
> 2. Learners' self-ratings of their agreement with important tenets of open science will increase over the course of the program (pre to post).
> 3. Change in 1 and 2 will be stronger for learners who report higher levels of engagement in the program. 

> All central hypotheses tested with mixed effects change-score models, with a random effect of cluster (change = post - pre), one model on change scores for ability and one on change scores for open science tenets. Another model including the effect of engagement as a covariate. For example:
> 
> `lme4::lmer(change ~ 1 + (1|cluster), data)`
> 
> `lme4::lmer(change ~ 1 + engagement + (1 + engagement|cluster), data)`
> 
> Follow up analyses will include each of the two engagement items as covariates separately. 
> 
> If models with random slopes do not converge, drop the random slopes and include only a random intercept for cluster.
>
> If random intercepts model does not converge, run linear model with cluster as a fixed effect:
>
> `lm(change ~ -1 + cluster, data = data)`
>
> `lm(change ~ cluster*engagement, data = data)`
>
> Initial plain dependent samples t-test ignoring grouping by cluster, mostly as descriptive analysis:
>
> `t.test(x=data$pre, y=data$post, paired = TRUE)`

### H1: Learners' self-ratings of their own ability to perform data science tasks will increase over the course of the program

```{r h1_test}
(h1_t <- t.test(x=data$ability_post, y=data$ability_pre, paired = TRUE))

mean(data$ability_change, na.rm = TRUE) # average change
sd(data$ability_change, na.rm = TRUE) # standard deviation

# hist of change scores
(data |> 
  dplyr::filter(!is.na(engagement)) |> 
  ggplot(aes(x=ability_change)) + 
  geom_histogram()) |> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "ability_change_hist_w1.png",
                 width = 6,
                 height = 3)
```

Random intercepts model:

```{r h1_lme}
h1_randomint <- lmerTest::lmer(ability_change ~ 1 + (1|pathway), data)
summary(h1_randomint)
```


### H2: Learners' self-ratings of their agreement with important tenets of open science will increase over the course of the program

```{r h2}
(h2_t <- t.test(x=data$openscience_post, y=data$openscience_pre, paired = TRUE))

mean(data$openscience_change, na.rm = TRUE) # average change
sd(data$openscience_change, na.rm = TRUE) # standard deviation

# hist of change scores
(data |> 
  dplyr::filter(!is.na(engagement)) |> 
  ggplot(aes(x=openscience_change)) + 
  geom_histogram()) |> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "openscience_change_hist_w1.png",
                 width = 6,
                 height = 3)
```

```{r h2_lme}
h2_randomint <- lme4::lmer(openscience_change ~ 1 + (1|pathway), data)
# boundary (singular) fit: see ?isSingular
```

```{r h2_lm}
h2_fe <- lm(openscience_change ~ -1 + pathway, data)
summary(h2_fe)
```

### H3: Change in 1 and 2 will be stronger for learners who report higher levels of engagement in the program

```{r h3}
h3_ab <- lm(ability_change ~ engagement, data = data)
summary(h3_ab)

h3_os <- lm(openscience_change ~ engagement, data = data)
summary(h3_os)
```

```{r}
# pathway as a fixed effect
h1_fe <- lm(ability_change ~ -1 + pathway, data = data)
h3_fe_ab <- lm(ability_change ~ -1 + pathway*engagement, data = data)
anova(h1_fe, h3_fe_ab)

h3_fe_os <- lm(openscience_change ~ -1 + pathway*engagement, data = data)
anova(h2_fe, h3_fe_os)
```

Random intercepts: 

```{r}
h3_randomint_ab <- lme4::lmer(ability_change ~ 1 + engagement + (1|pathway), data) # fails to converge

h3_randomint_os <- lme4::lmer(openscience_change ~ 1 + engagement + (1|pathway), data) # singular
```


Random slopes model:

```{r}
h3_randomslope <- lme4::lmer(ability_change ~ 1 + engagement + (1 + engagement|pathway), data) # singular fit (i.e. we can't estimate this model)

h3_randomslope_os <- lme4::lmer(openscience_change ~ 1 + engagement + (1 + engagement|pathway), data) # singular fit (i.e. we can't estimate this model)


# drop participants from green pathway (n=1) and indigo (n=2)
h3_randomslope <- lme4::lmer(ability_change ~ 1 + engagement + (1 + engagement|pathway), dplyr::filter(data, pathway != "green" & pathway != "indigo")) # singular fit

# drop participants from green pathway (n=1) and indigo (n=2)
h3_randomslope_os <- lme4::lmer(openscience_change ~ 1 + engagement + (1 + engagement|pathway), dplyr::filter(data, pathway != "green" & pathway != "indigo")) # singular fit
```


## tldr

```{r helper_functions, echo=FALSE}
print_r <- function(r, digits=2){
  format <- paste0("%0.", digits, "f")
  sub(pattern = "^0", replacement = "", x=sprintf(format, r))
}

print_p <- function(p, digits = 3){
  if (p < .001){
    p_statement <- "p < .001"
  } else {
    format <- paste0("%0.", digits, "f")
    p_statement <- paste0("p = ", sub(pattern = "^0", replacement = "", x=sprintf(format, p)))
  }
}

print_lm <- function(model, digits = 2, digits_p = 3){
  
  format <- paste0("%0.", digits, "f")
  
  rsq <- summary(model)$r.squared
  f <- summary(model)$fstatistic["value"]
  df1 <- summary(model)$fstatistic["numdf"]
  df2 <- summary(model)$fstatistic["dendf"]
  p <- pf(f,df1,df2, lower.tail = FALSE)
  paste0("RÂ² = ", print_r(rsq, digits), ", F(", df1, ", ", df2, ") = ", sprintf(format, f), ", ", print_p(p, digits_p))
}

print_t <- function(ttest, digits = 2, digits_p = 3){
  
  format <- paste0("%0.", digits, "f")
  
  paste0("t(", ttest$parameter, ") = ", sprintf(format, ttest$statistic), ", ", print_p(ttest$p.value, digits_p) )
}

print_anova <- function(anova, digits=2, digits_p=3){
  if(nrow(anova) != 2) message("Only enter two models at a time.")
  stopifnot(nrow(anova) == 2)
  
  format <- paste0("%0.", digits, "f")
  
  df1 <- anova$Df[2]
  df2 <- anova$Res.Df[2]
  f <- anova$F[2]
  p <- anova$`Pr(>F)`[2]
  paste0("F(", df1, ", ", df2, ") = ", sprintf(format, f), ", ", print_p(p, digits_p))
}

print_coef <- function(model, coef, digits = 2, digits_p = 3){
  b <- summary(model)$coefficients[coef, "Estimate"]
  t <- summary(model)$coefficients[coef, "t value"]
  p <- summary(model)$coefficients[coef, "Pr(>|t|)"]
  
  if("df" %in% colnames(summary(model)$coefficients)){
    # for lmerTest models
    df <- summary(model)$coefficients[coef, "df"]
  } else if (class(model) == "lm") {
    # for lm models
    df <- model$df.residual
  } else stop("Cannot recover df for coefficient test.")
  
  format <- paste0("%0.", digits, "f")
  
  result <- paste0("b = ", round(b, digits), ", t(", round(df, digits), ") = ", sprintf(format, t), ", ", print_p(p, digits_p) )
  
  return(result)
}
```


Our hypotheses, from the preregistration: 

> 1. Learners' self-ratings of their own ability to perform data science tasks will increase over the course of the program (pre to post).
> 2. Learners' self-ratings of their agreement with important tenets of open science will increase over the course of the program (pre to post).
> 3. Change in 1 and 2 will be stronger for learners who report higher levels of engagement in the program. 

```{r, echo=FALSE}
ab_change_mean <- mean(data$ability_change, na.rm = TRUE) # average change
ab_change_sd <- sd(data$ability_change, na.rm = TRUE) # standard deviation 

os_change_mean <- mean(data$openscience_change, na.rm = TRUE) # average change
os_change_sd <- sd(data$openscience_change, na.rm = TRUE) # standard deviation 
```

H1 is supported by the data. A random intercepts model with pathway as random effect shows a significant improvement in participants' self-rated ability on data science tasks from pretest to post (`r print_coef(h1_randomint, "(Intercept)")`). 
A paired t-test, ignoring the grouping structure altogether, also shows a significant improvement in participants' self-rated ability on data science tasks from pretest to post, `r print_t(h1_t)` (mean (SD) change is `r round(ab_change_mean, 2)` (`r round(ab_change_sd, 2)`) on a 4-point scale from 1 "I wouldn't know where to start" to 4 "I am confident in my ability to do it").



H2 is not clearly supported by the data, although there's some suggestion of a trend in the predicted direction. 
The planned random intercepts model was singular and therefore couldn't be estimated. 
Our pre-registered contingency plan in this case was to run a linear model with pathway as a fixed effect instead, which shows no significant improvement in agreement with open science values in any of the 8 pathways `r print_lm(h2_fe)`. 
A paired t-test, ignoring the grouping structure altogether, shows a significant improvement in participants' self-rated level of agreement with open science values from pretest to post, `r print_t(h2_t)` (mean (SD) change is `r round(os_change_mean, 2)` (`r round(os_change_sd, 2)`) on a 7-point scale from 1 "strongly disagree" to 7 "strongly agree"). 
An examination of the raw scores for open science items reveals a probable ceiling effect; the mean open science score at pretest was already `r round(mean(data$openscience_pre), 2)` on a scale from 1 to 7, so there was no room to improve for many participants.  

```{r, results='asis', echo=FALSE, message=FALSE}
#stargazer(h2_fe, type = "html")
```

H3 is partially supported in the data. 
Engagement significantly predicts change in self-rated ability (`r print_lm(h3_ab)`) but not change in level of agreement with open science values (`r print_lm(h3_os)`), ignoring the grouping by pathways. 
A linear model with pathway as a fixed effect shows no significant effect of engagement on open science values for any of the pathways (`r print_anova(anova(h2_fe, h3_fe_os))`), but there is some evidence of an effect of engagement on change in ability in a model with pathway as a fixed effect (`r print_anova(anova(h1_fe, h3_fe_ab))`). 
This result is ambiguous, however, because the significant interaction between engagement and pathway appears to be driven by respondents in a single pathway ("teal" pathway, n=`r nrow(dplyr::filter(data, pathway == "teal"))`), rather than being a more general effect.   
Mixed effects models including engagement (with random intercepts only, or with random intercepts and random slopes for engagement) do not converge for either the ability or open science outcome. 

One possible explanation of the lack of an effect of engagement is the quality of the engagement index itself. 
It is the average of participants' reported amount of interaction with other participants (`interaction_how_often`) and amount of completed modules (`completed_modules`), but overall reported interaction with other participants was very low (mean = `r round(mean(data$interaction_how_often), 2)` on a scale of 1=Never, 2=Very little, 3=Sometimes, 4=Frequently, 5=Very frequently) and the correlation between completed modules and interaction with other participants was small (r=`r print_r(cor(data$interaction_how_often, data$completed_modules))`). 
We had expected interaction with other participants and completion of modules to be highly correlated, both indicating an overall level of engagement with the program, but that is not the case.
Qualitative feedback from some participants speaks to frustration with a lack opportunities to connect with other participants, and a lack of response from other participants when interaction was attempted; 
these barriers to interaction may have undermined the effectiveness of that measure as an indicator of engagement.
In retrospect, using `completed_modules` alone may have been a better proxy for participants' engagement in the program. 
An alternative approach would be to build a more complete model of the engagement construct, with more measures capturing engagement and better internal consistency among items. 

## Discussion

### Plots for hypothesis testing

```{r plot_data}
plot_data <- data |> 
  tidyr::pivot_longer(cols=tidyselect::starts_with(c("ability","openscience"))) |> 
  tidyr::separate(name, into=c("measure", "event"), sep="_") |> 
  tidyr::pivot_wider(names_from = measure, values_from = value) |> 
  dplyr::mutate(event = factor(event, levels = c("pre", "post", "change")))

plot_data_means <- plot_data |> 
  dplyr::filter(event %in% c("pre", "post")) |> 
  dplyr::group_by(event) |> 
  dplyr::summarise(ability = mean(ability), 
                   openscience = mean(openscience))
```

```{r define_base_plots}
ability_plot <- plot_data |> 
  dplyr::filter(event %in% c("pre", "post")) |> 
  ggplot(aes(y=ability)) + 
  scale_y_continuous(breaks = 1:4, limits = c(1,4),
                     labels = c("(1) I wouldn't know where to start",
                                "(2) I could struggle through,\nbut not confident I could do it",
                                "(3) I could probably do it\nwith some trial and error",
                                "(4) I am confident in my ability to do it")) + 
  labs(x=NULL, y=NULL) + 
   theme(
    strip.text = element_text(face = "bold"),
    strip.background = element_rect(colour = "black", linewidth = 1))

openscience_plot <- plot_data |> 
  dplyr::filter(event %in% c("pre", "post")) |> 
  ggplot(aes(y=openscience)) + 
  scale_y_continuous(breaks = 1:7, limits = c(1,7),
                     labels = c("(1) Strongly Disagree",
                                "(2) Disagree",
                                "(3) Somewhat Disagree",
                                "(4) Neither Agree nor Disagree",
                                "(5) Somewhat Agree",
                                "(6) Agree", 
                                "(7) Strongly Agree")) + 
  labs(x=NULL, y=NULL) + 
   theme(
    strip.text = element_text(face = "bold"),
    strip.background = element_rect(colour = "black", linewidth = 1))
```

```{r pre_post_ability}
(ability_plot + 
  geom_histogram(bins = 30) + 
  facet_wrap(~event, ncol = 2) 
 ) |> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "ability_pre_post_hist_w1.tiff",
                 width = 6,
                 height = 3)
```

```{r pre_post_openscience}
(openscience_plot + 
  geom_histogram() + 
  facet_wrap(~event, ncol = 2)) |> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "openscience_pre_post_hist_w1.tiff",
                 width = 6,
                 height = 3)
```


### Additional plots

How about those engagement scores?

- `interaction_how_often` = How often did you engage with other DART participants in the program? (1, Never | 2, Very little | 3, Sometimes | 4, Frequently | 5, Very frequently)
- `completed_modules` = How many of the assigned learning modules on your pathway did you complete? (1, None at all | 2, A few | 3, About half | 4, Most | 5, All)

```{r engagement}
(engagement |> 
  tidyr::pivot_longer(cols = -record_id) |> 
  ggplot(aes(x=value)) + 
  geom_histogram(bins=5) + 
  facet_wrap(~name, ncol=1)) |> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "engagement_hist_w1.png",
                 width = 5,
                 height = 5)

(engagement |> 
  ggplot(aes(x=completed_modules, y=interaction_how_often)) + 
  geom_jitter(width = .2, height = .2, alpha=.5) + 
  geom_rug(position = "jitter", alpha=.5) + 
  stat_smooth(method = "lm")) |> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "engagement_scatter_w1.png",
                 width = 5,
                 height = 5)
```




```{r change_plots}
# 1, I wouldn't know where to start | 2, I could struggle through, but not confident I could do it | 3, I could probably do it with some trial and error | 4, I am confident in my ability to do it	
(plot_data |> 
  dplyr::filter(event %in% c("pre", "post")) |> 
  ggplot(aes(x=event, y=ability, group = record_id)) + 
  geom_line(alpha = .5) + 
  scale_y_continuous(breaks = 1:4, limits = c(1,4)) + 
  labs(x=NULL)) |> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "ability_change_lines_w1.png",
                 width = 6,
                 height = 3)

# 1, strongly disagree | 2, disagree | 3, somewhat disagree | 4, neither agree nor disagree | 5, somewhat agree | 6, agree | 7, strongly agree
(plot_data |> 
  dplyr::filter(event %in% c("pre", "post")) |> 
  ggplot(aes(x=event, y=openscience, group = record_id)) + 
  geom_line(alpha = .5) + 
  scale_y_continuous(breaks = 1:7, limits = c(1,7)) + 
  labs(x=NULL)) |> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "openscience_change_lines_w1.png",
                 width = 6,
                 height = 3)
```

```{r change_plots_by_pathway}
# 1, I wouldn't know where to start | 2, I could struggle through, but not confident I could do it | 3, I could probably do it with some trial and error | 4, I am confident in my ability to do it	
(plot_data |> 
  dplyr::filter(event %in% c("pre", "post")) |> 
  ggplot(aes(x=event, y=ability, group = record_id)) + 
  geom_line(alpha = .5) + 
  scale_y_continuous(breaks = 1:4, limits = c(1,4)) + 
  labs(x=NULL) + 
  facet_wrap(~pathway)) |> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "ability_change_by_pathway_w1.png",
                 width = 6,
                 height = 6)

# 1, strongly disagree | 2, disagree | 3, somewhat disagree | 4, neither agree nor disagree | 5, somewhat agree | 6, agree | 7, strongly agree
(plot_data |> 
  dplyr::filter(event %in% c("pre", "post")) |> 
  ggplot(aes(x=event, y=openscience, group = record_id)) + 
  geom_line(alpha = .5) + 
  scale_y_continuous(breaks = 1:7, limits = c(1,7)) + 
  labs(x=NULL) + 
  facet_wrap(~pathway))|> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "openscience_change_by_pathway_w1.png",
                 width = 6,
                 height = 6)
```

### n per cluster

```{r}
# only n=1 in green pathway, n=2 in indigo
data |> 
  dplyr::count(pathway) |> 
  knitr::kable()
```

In order to estimate models with pathway as a random effect, we need bigger n in all groups. 

