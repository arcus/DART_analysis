---
title: "Wave 2 Hypothesis Testing"
author: "Rose Hartman"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      out.width = "50%")

library(ggplot2)

# load custom functions for plotting
source(here::here("src", "scripts", "functions_plotting.R"))

# if the figures directory doesn't exist, create it
dir.create(here::here("reports"), showWarnings = FALSE)
dir.create(here::here("reports", "figures"), showWarnings = FALSE)
dir.create(here::here("reports", "tables"), showWarnings = FALSE)
```

`r if(!knitr::opts_chunk$get()$echo) ">Note that code chunks are not printed in this report in order to keep the output tidy. To see all of the code to generate these results, open the .Rmd file."`

This report executes the tests from [our preregistration](https://osf.io/zmnr6). 
Text that appears as quotes in this report is taken verbatim from the preregistration.

## Hypotheses to test

> 1. Learners' self-ratings of their own ability to perform data science tasks will increase over the course of the program (pre to post).
> 2. Learners' self-ratings of their agreement with important tenets of open science will increase over the course of the program (pre to post).
> 3. Change in 1 and 2 will be stronger for learners who report higher levels of engagement in the program. 
> 
> Additional hypotheses:
>
> 4. Change in 1 and 2 will be higher for learners who report higher levels of agreement with the statement "Self-paced asynchronous studying works well for me in general"
> 5. Change in 1 and 2 will be higher for learners who report higher average levels of agreement with the statements about the appropriateness of their assigned pathway ("The skills and topics I was hoping to learn were covered in my modules", "My assigned modules worked together well as a learning pathway", "My assigned modules were appropriate to my skill level", "I learned things from my assigned modules that I can apply in my research", and "The assigned modules for my pathway were relevant to my learning goals")


```{r load_data}
waves <- readr::read_csv(here::here("participant_waves.csv"), show_col_types = FALSE)

nih_pre <- readRDS(here::here("data", "deidentified", "nih_pre.rds")) |> 
  dplyr::left_join(waves, by = "record_id") |> 
  dplyr::filter(wave == 2) |> 
  dplyr::mutate(event = "pre")
nih_post <- readRDS(here::here("data", "deidentified", "nih_post.rds")) |> 
  dplyr::left_join(waves, by = "record_id") |> 
  dplyr::filter(wave == 2)|> 
  dplyr::mutate(event = "post")
nih <- dplyr::bind_rows(nih_post, nih_pre)

exit <- readRDS(here::here("data", "deidentified", "exit_survey.rds")) |> 
  dplyr::left_join(waves, by = "record_id") |> 
  dplyr::filter(wave == 2)

nalms <- readRDS(here::here("data", "deidentified", "nalms_summary.rds")) |>
  dplyr::left_join(waves, by = "record_id") |> 
  dplyr::filter(wave == 2) |> 
  dplyr::left_join(readRDS(here::here("data", "deidentified", "nalms_pathway_info.rds")), by = "pathway")
```

This data includes `r nrow(nih_pre)` records for the NIH items (ability and open science variables) at pretest, and `r nrow(nih_post)` at post test. 
There are `r nrow(exit)` exit survey records (asynchronous learning preference and pathway fit variables).
There are `r length(unique(nalms$record_id))` NALMS records (used to calculate engagement), but note that only `r length(unique(dplyr::filter(nalms, redcap_repeat_instance > 1)$record_id))` actually show any activity from participants.

## Define variables

From our preregistration:

> For all mean scores, the mean will be computed on available data (i.e. using na.rm = TRUE).

> ability_pre and ability_post are the mean of participants' ratings (assessed pre- or post-program, respectively) on the following items from survey2: all items under the instructions "please rate your ability to complete the following tasks". 
>
> openscience_pre and openscience_post are the mean of participants' ratings (assessed pre- or post-program, respectively) on the following items from survey2: all items under the heading "please rate your level of agreement with the following statements"

```{r}
data <- nih |> 
  dplyr::mutate(dplyr::across(tidyselect::where(is.factor), as.numeric)) |> 
  dplyr::rowwise() |> 
  dplyr::mutate(ability = mean(dplyr::c_across(findable:atomize), na.rm = TRUE),
                openscience = mean(dplyr::c_across(data_storage_1:code_efficient), na.rm = TRUE)) |> 
  dplyr::select(record_id, ability, openscience, event) |> 
  tidyr::pivot_wider(names_from = event, values_from = c(ability, openscience)) |> 
  dplyr::mutate(ability_change = ability_post - ability_pre, 
                openscience_change = openscience_post - openscience_pre)
```

> Engagement is defined as number of modules marked complete out of total modules assigned as of the last day of the program (12am Eastern 2023-11-27). 

```{r}
engagement <- nalms |> 
  dplyr::filter(timestamp < lubridate::ymd("2023-11-27")) |> 
  dplyr::group_by(record_id) |> 
  dplyr::slice_max(timestamp) |> 
  dplyr::mutate(engagement = n_modules_done / pathway_n_modules) |> 
  dplyr::select(record_id, engagement, pathway)

# add engagement to data
data <- dplyr::left_join(data, engagement, by = "record_id")
```


> Asynchronous learning preference is participants' level of agreement (1strongly disagree - 5 strongly agree) with the statement "Self-paced asynchronous studying works well for me in general". 

> Pathway fit is the mean of participants' level of agreement (1strongly disagree - 5 strongly agree) with the following five statements: "The skills and topics I was hoping to learn were covered in my modules", "My assigned modules worked together well as a learning pathway", "My assigned modules were appropriate to my skill level", "I learned things from my assigned modules that I can apply in my research", "The assigned modules for my pathway were relevant to my learning goals")

```{r}
asynch_and_pathway_fit <- exit |> 
  dplyr::mutate(dplyr::across(tidyselect::where(is.factor), as.numeric)) |> 
  dplyr::rowwise() |> 
  dplyr::mutate(pathway_fit = mean(c(module_expectations, fit_expertise, coherent_pathway, fit_relevance, fit_learn), na.rm = TRUE)) |> 
  dplyr::select(record_id, pathway_fit, asynch)

# Add exit data 
data <- dplyr::left_join(data, asynch_and_pathway_fit, by = "record_id")
```

Here is a quick summary of the fields that will be used in this analysis (scroll right to see the rest):

```{r, results='asis'}
# reorder columns, just for nicer printing
data <- data |> 
  dplyr::select(record_id, pathway, tidyselect::everything()) |> 
  # only keep records where we have data on all the fields needed
  na.omit()

options(knitr.kable.NA = "")
# show a summary
summary(data) |> 
  knitr::kable()
```


# Preregistered hypothesis tests

From our preregistration:

> All central hypotheses tested with mixed effects change-score models, with a random effect of cluster (change = post - pre), one model on change scores for ability and one on change scores for open science tenets. Another model including the effect of moderators as a covariate. For example:
>
> ```
> lme4::lmer(change ~ 1 + (1|cluster), data)
> lme4::lmer(change ~ 1 + engagement + (1 + engagement|cluster), data)
> lme4::lmer(change ~ 1 + asynch + (1 + asynch|cluster), data)
> lme4::lmer(change ~ 1 + pathway_fit + (1 + pathway_fit|cluster), data)
> ```
> 
> If models with random slopes do not converge, drop the random slopes and include only a random intercept for cluster. 
> 
> If random intercepts model does not converge, run linear model with cluster as a fixed effect, and omitting cluster altogether:
> 
> ```
> lm(change ~ cluster, data = data)
> lm(change ~ 1, data = data)
> lm(change ~ cluster*engagement, data = data)
> lm(change ~ engagement, data = data)
> lm(change ~ cluster*asynch, data = data)
> lm(change ~ asynch, data = data)
> lm(change ~ cluster*pathway_fit, data = data)
> lm(change ~ pathway_fit, data = data)
> ```
> 
> Whether or not mixed models work, also run plain dependent samples t-test ignoring grouping by cluster:
> 
> ```
> t.test(x=data$pre, y=data$post, paired = TRUE)
> ```

> p values for mixed effects models calculated using Satterthwaite approximation (lmerTest package in R)

```{r load_lmerTest}
library(lmerTest)
# note that lmerTest::lmer() is the same as lme4::lmer except that it adds some slots needed for the computation of Satterthwaite denominator degrees of freedom
# see ?lmerTest::lmer
```


## Hypothesis 1

> 1. Learners' self-ratings of their own ability to perform data science tasks will increase over the course of the program (pre to post).

```{r}
h1 <- lmerTest::lmer(ability_change ~ 1+ (1|pathway), data)
summary(h1)

(h1_t <- t.test(y=data$ability_pre, x=data$ability_post, paired = TRUE))
```

## Hypothesis 2

> 2. Learners' self-ratings of their agreement with important tenets of open science will increase over the course of the program (pre to post).

```{r}
h2 <- lmerTest::lmer(openscience_change ~ 1+ (1|pathway), data)
summary(h2)

(h2_t <- t.test(y=data$openscience_pre, x=data$openscience_post, paired = TRUE))
```

## Hypothesis 3

> 3. Change in 1 and 2 will be stronger for learners who report higher levels of engagement in the program.

```{r h3_ability}
h3_ability <- lmerTest::lmer(ability_change ~ 1 + engagement + (1+ engagement|pathway), data)
summary(h3_ability)

(h3_ability_aov <- anova(h3_ability, h1))
```

```{r h3_openscience}
h3_openscience <- lmerTest::lmer(openscience_change ~ 1 + engagement + (1+ engagement|pathway), data) # boundary (singular) fit: see ?isSingular
h3_openscience <- lmerTest::lmer(openscience_change ~ 1 + engagement + (1|pathway), data)
summary(h3_openscience)

(h3_openscience_aov <- anova(h3_openscience, h2))
```

## Hypothesis 4

> 4. Change in 1 and 2 will be higher for learners who report higher levels of agreement with the statement "Self-paced asynchronous studying works well for me in general"

```{r h4_ability}
h4_ability <- lmerTest::lmer(ability_change ~ 1 + asynch + (1 + asynch|pathway), data)
h4_ability <- lmerTest::lmer(ability_change ~ 1 + asynch + (1 |pathway), data)
summary(h4_ability)

(h4_ability_aov <- anova(h4_ability, h1))
```

```{r h4_openscience}
h4_openscience <- lmerTest::lmer(openscience_change ~ 1 + asynch + (1 |pathway), data)
summary(h4_openscience)

(h4_openscience_aov <- anova(h4_openscience, h2))
```

## Hypothesis 5

> 5. Change in 1 and 2 will be higher for learners who report higher average levels of agreement with the statements about the appropriateness of their assigned pathway ("The skills and topics I was hoping to learn were covered in my modules", "My assigned modules worked together well as a learning pathway", "My assigned modules were appropriate to my skill level", "I learned things from my assigned modules that I can apply in my research", and "The assigned modules for my pathway were relevant to my learning goals")

```{r h5_ability}
h5_ability <- lmerTest::lmer(ability_change ~ 1 + pathway_fit + (1 + pathway_fit|pathway), data)
h5_ability <- lmerTest::lmer(ability_change ~ 1 + pathway_fit + (1 |pathway), data)
summary(h5_ability)

(h5_ability_aov <- anova(h5_ability, h1))
```

```{r h5_openscience}
h5_openscience <- lmerTest::lmer(openscience_change ~ 1 + pathway_fit + (1 + pathway_fit|pathway), data)
h5_openscience <- lmerTest::lmer(openscience_change ~ 1 + pathway_fit + (1|pathway), data)
summary(h5_openscience)

(h5_openscience_aov <- anova(h5_openscience, h2))
```

# Plots

```{r plot_data}
plot_data <- data |> 
  tidyr::pivot_longer(cols=tidyselect::starts_with(c("ability","openscience"))) |> 
  tidyr::separate(name, into=c("measure", "event"), sep="_") |> 
  tidyr::pivot_wider(names_from = measure, values_from = value) |> 
  dplyr::mutate(event = factor(event, levels = c("pre", "post", "change")))

plot_data_means <- plot_data |> 
  dplyr::filter(event %in% c("pre", "post")) |> 
  dplyr::group_by(event) |> 
  dplyr::summarise(ability = mean(ability), 
                   openscience = mean(openscience))
```

```{r define_base_plots}
ability_plot <- plot_data |> 
  dplyr::filter(event %in% c("pre", "post")) |> 
  ggplot(aes(y=ability)) + 
  scale_y_continuous(breaks = 1:4, limits = c(1,4),
                     labels = c("(1) I wouldn't know where to start",
                                "(2) I could struggle through,\nbut not confident I could do it",
                                "(3) I could probably do it\nwith some trial and error",
                                "(4) I am confident in my ability to do it")) + 
  labs(x=NULL, y=NULL, 
       title = "Ability")

openscience_plot <- plot_data |> 
  dplyr::filter(event %in% c("pre", "post")) |> 
  ggplot(aes(y=openscience)) + 
  scale_y_continuous(breaks = 1:7, limits = c(1,7),
                     labels = c("(1) Strongly Disagree",
                                "(2) Disagree",
                                "(3) Somewhat Disagree",
                                "(4) Neither Agree nor Disagree",
                                "(5) Somewhat Agree",
                                "(6) Agree", 
                                "(7) Strongly Agree")) + 
  labs(x=NULL, y=NULL, 
       title = "Open Science Values")
```


## Distributions and summary stats for all key variables

```{r n_per_pathway}
data |> 
  dplyr::select(pathway) |> 
  gtsummary::tbl_summary() |> 
  gtsummary::as_gt() |> # convert to gt table
  gt::gtsave( # save table as image
    filename = "pathway_n.png",
    path = here::here("reports", "tables"))

knitr::include_graphics(here::here("reports", "tables", "pathway_n.png"))
```

```{r summary_stats_tables}
# outcomes
data |> 
  dplyr::select(record_id, 
                ability_pre, ability_post, ability_change, 
                openscience_pre, openscience_post, openscience_change) |>
  tidyr::pivot_longer(-record_id) |> 
  tidyr::separate(col = name, into = c("name", "event"), sep = "_") |> 
  tidyr::pivot_wider(names_from = name, values_from = value) |> 
  dplyr::select(-record_id) |> 
  gtsummary::tbl_summary(by = event) |> 
  gtsummary::as_gt() |> # convert to gt table
  gt::gtsave( # save table as image
    filename = "descriptives_outcomes.png",
    path = here::here("reports", "tables"))

knitr::include_graphics(here::here("reports", "tables", "descriptives_outcomes.png"))

# covariates
data |> 
  dplyr::select(engagement, pathway_fit, asynch) |>
  gtsummary::tbl_summary(type = list(asynch ~ "continuous")) |> 
  gtsummary::as_gt() |> # convert to gt table
  gt::gtsave( # save table as image
    filename = "descriptives_covariates.png",
    path = here::here("reports", "tables"))

knitr::include_graphics(here::here("reports", "tables", "descriptives_covariates.png"))
```

```{r}
(ability_plot + 
  geom_histogram(bins = 30) + 
  coord_flip() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust=1)) + 
  facet_wrap(~event, ncol = 1)) |> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "ability_pre_post_hist.png",
                 width = 3,
                 height = 7)
```



```{r}
(openscience_plot + 
  geom_histogram() + 
  coord_flip() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust=1)) + 
  facet_wrap(~event, ncol = 1)) |> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "openscience_pre_post_hist.png",
                 width = 3,
                 height = 7)
```

```{r}
(data |> 
  tidyr::pivot_longer(tidyselect::ends_with("_change")) |> 
  ggplot(aes(x=value)) + 
  geom_histogram() + 
  facet_wrap(~name, ncol = 1) + 
  geom_vline(xintercept = 0, linetype = 2)) |> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "change_hist.png",
                 width = 3,
                 height = 5)
```


## Change in ability from pre to post

```{r}
(ability_plot + 
  geom_boxplot(aes(x=event), fill = chop_blue)) |> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "ability_boxplot.png",
                 width = 5,
                 height = 3.5)

(ability_plot + 
  geom_line(aes(x=event, group = record_id), alpha = .4)) |> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "ability_lineplot.png",
                 width = 5,
                 height = 3.5)
```


Let's look at the relationship between engagement and change in ability by pathway.

```{r}
(data |> 
  ggplot(aes(x = engagement, y = ability_change, color = pathway)) + 
  geom_point(show.legend = FALSE) + 
  stat_smooth(method = "lm") + 
  facet_wrap(~pathway)) |> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "ability_change_by_pathway.png",
                 width = 5,
                 height = 5)
```


## Change in open science from pre to post

```{r}
(openscience_plot + 
  geom_boxplot(aes(x=event), fill = chop_blue)) |> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "openscience_boxplot.png",
                 width = 5,
                 height = 3.5)

(openscience_plot + 
  geom_line(aes(x=event, group = record_id), alpha = .4)) |> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "openscience_lineplot.png",
                 width = 5,
                 height = 3.5)
```


Let's look at the relationship between engagement and change in open science by pathway.

```{r}
(data |> 
  ggplot(aes(x = engagement, y = openscience_change, color = pathway)) + 
  geom_point(show.legend = FALSE) + 
  stat_smooth(method = "lm") + 
  facet_wrap(~pathway)) |> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "openscience_change_by_pathway.png",
                 width = 5,
                 height = 5)
```


```{r}
(data |> 
  ggplot(aes(y=ability_change, x = openscience_change))  +
  geom_point(alpha = .5)) |> 
  # the save_and_print function is defined in functions_plotting.R
  save_and_print(filename = "openscience_change_by_ability_change.png",
                 width = 5,
                 height = 5)
```

# Executive summary

```{r helper_functions}
print_r <- function(r, digits=2){
  format <- paste0("%0.", digits, "f")
  sub(pattern = "^0", replacement = "", x=sprintf(format, r))
}

print_p <- function(p, digits = 3){
  if (p < .001){
    p_statement <- "p < .001"
  } else {
    format <- paste0("%0.", digits, "f")
    p_statement <- paste0("p = ", sub(pattern = "^0", replacement = "", x=sprintf(format, p)))
  }
}

print_lm <- function(model, digits = 2, digits_p = 3){
  
  format <- paste0("%0.", digits, "f")
  
  rsq <- summary(model)$r.squared
  f <- summary(model)$fstatistic["value"]
  df1 <- summary(model)$fstatistic["numdf"]
  df2 <- summary(model)$fstatistic["dendf"]
  p <- pf(f,df1,df2, lower.tail = FALSE)
  paste0("R² = ", print_r(rsq, digits), ", F(", df1, ", ", df2, ") = ", sprintf(format, f), ", ", print_p(p, digits_p))
}

print_t <- function(ttest, digits = 2, digits_p = 3){
  
  format <- paste0("%0.", digits, "f")
  
  paste0("t(", ttest$parameter, ") = ", sprintf(format, ttest$statistic), ", ", print_p(ttest$p.value, digits_p) )
}

print_anova <- function(anova, digits=2, digits_p=3, n=NULL){
  # good reference: https://online.stat.psu.edu/stat504/lesson/6/6.3/6.3.4
  
  if(nrow(anova) != 2) stop("Only enter two models at a time.")
  
  format <- paste0("%0.", digits, "f")
  
  if ("F" %in% colnames(anova)){
    # for F tests comparing the R-sq of two linear models
    df1 <- anova$Df[2]
    df2 <- anova$Res.Df[2]
    f <- anova$F[2]
    p <- anova$`Pr(>F)`[2]
    result <- paste0("F(", df1, ", ", df2, ") = ", sprintf(format, f), ", ", print_p(p, digits_p))
  } else if ("Chisq" %in% colnames(anova)){
    # for chi-sq tests comparing the deviance of two linear models
    
    if(is.null(n)) stop("You must provide N for analysis of deviance.")
    
    df <- anova$Df[2]
    chisq <- anova$Chisq[2]
    p <- anova$`Pr(>Chisq)`[2]
    result <- paste0("χ2 (", df, ", N=", n, ") = ", sprintf(format, chisq), ", ", print_p(p, digits_p))
  } else stop("ERROR: Unrecognized model comparison")
  
  return(result)
}

print_coef <- function(model, coef, digits = 2, digits_p = 3){
  b <- summary(model)$coefficients[coef, "Estimate"]
  t <- summary(model)$coefficients[coef, "t value"]
  p <- summary(model)$coefficients[coef, "Pr(>|t|)"]
  
  if("df" %in% colnames(summary(model)$coefficients)){
    # for lmerTest models
    df <- summary(model)$coefficients[coef, "df"]
  } else if (class(model) == "lm") {
    # for lm models
    df <- model$df.residual
  } else stop("Cannot recover df for coefficient test.")
  
  format <- paste0("%0.", digits, "f")
  
  result <- paste0("b = ", round(b, digits), ", t(", round(df, digits), ") = ", sprintf(format, t), ", ", print_p(p, digits_p) )
  
  return(result)
}
```

```{r}
ab_change_mean <- mean(data$ability_change, na.rm = TRUE) # average change
ab_change_sd <- sd(data$ability_change, na.rm = TRUE) # standard deviation 

os_change_mean <- mean(data$openscience_change, na.rm = TRUE) # average change
os_change_sd <- sd(data$openscience_change, na.rm = TRUE) # standard deviation 
```

We collected participants' ratings of their own current level of ability in a variety of data science skills (e.g. "Build a data processing pipeline that can be used in multiple programs"), and also their level of agreement with important tenets of open science and reproducibility (e.g. "Open and efficient data sharing is vital to the advancement of the field"). 
As per our preregistration, participants with missing data on any of the analysis variables are dropped (listwise deletion), resulting in `r nrow(data)` complete cases for analysis. 

There is strong support for the hypothesis that learners' ability improved over the DART program (Hypothesis 1).
A random intercepts model with pathway as random effect shows a significant improvement in participants' self-rated ability on data science tasks from pretest to post, `r print_coef(h1, "(Intercept)")` (all coefficient tests for mixed effects models are reported using Satterthwaite's approximation for degrees of freedom). 
A paired t-test, ignoring the grouping structure altogether, also shows a significant improvement in participants' self-rated ability on data science tasks from pretest to post, `r print_t(h1_t)`, mean (SD) change is `r round(ab_change_mean, 2)` (`r round(ab_change_sd, 2)`) on a 4-point scale from 1 "I wouldn't know where to start" to 4 "I am confident in my ability to do it", an effect size of $d$ = `r round(ab_change_mean/ab_change_sd, 2)`.

We also saw significant increase in participants' level of agreement with open science values (Hypothesis 2), `r print_coef(h2, "(Intercept)")`, but with a more modest effect size than observed with the ability ratings.
A paired t-test, ignoring the grouping structure altogether, shows a significant improvement in participants' self-rated level of agreement with open science values from pretest to post, `r print_t(h2_t)`, mean (SD) change is `r round(os_change_mean, 2)` (`r round(os_change_sd, 2)`) on a 7-point scale from 1 "strongly disagree" to 7 "strongly agree", an effect size of $d$ = `r round(os_change_mean/os_change_sd, 2)`. 
An examination of the raw scores for open science items reveals a probable ceiling effect; the mean open science score at pretest was already `r round(mean(data$openscience_pre), 2)` on a scale from 1 to 7, so there was no room to improve for many participants.  

```{r, message=FALSE}
library(stargazer)
# need to re-save models with class(model) <- "lmerMod" so stargazer can process them
class(h1) <- "lmerMod"
class(h2) <- "lmerMod"
class(h3_ability) <- "lmerMod"
class(h3_openscience) <- "lmerMod"
class(h4_ability) <- "lmerMod"
class(h4_openscience) <- "lmerMod"
class(h5_ability) <- "lmerMod"
class(h5_openscience) <- "lmerMod"
```

```{r, results='asis'}
stargazer(h1, h2, type="html")
```

We can learn more about the nature of the gains observed over the course of participation in DART by controlling for relevant covariates. 
In particular, if the DART program itself is driving gains, we would expect to see more improvement for participants that engaged more in the program, i.e. completed more of their assigned modules (Hypothesis 3). 
We can test the predictive power of engagement by re-running the mixed effects models above with engagement added as a predictor. 
Our preregistration specifies that we also allow a random slope for each covariate, with the backup plan that if the random slopes model doesn't converge we revert to a random intercept only; we were able to include a random slope for engagement in the model predicting change in ability, but only a random intercept in the model predicting change in open science values.
Percent of assigned modules completed ("engagement") does significantly predict change in ability from pre to post, `r print_anova(h3_ability_aov, n=nrow(model.frame(h1)))`.
However, engagement does not significantly predict change in agreement with open science values, `r print_anova(h3_openscience_aov, n=nrow(model.frame(h1)))`. 
See table below for coefficient estimates. 

```{r, results='asis', message=FALSE}
library(stargazer)
stargazer(h3_ability, h3_openscience, type="html")
```

Similarly, we might expect that a program like DART would work better for learners who report generally doing well with asynchronous education; their preference might allow them to get more from the program, increasing its effect. 
Indeed, the degree to which participants agree with the statement "Self-paced asynchronous studying works well for me in general" significantly predicts change in ability, `r print_anova(h4_ability_aov, n=nrow(model.frame(h1)))`.
Preference for asynchronous learning does not significantly predict change in open science values, though, `r print_anova(h4_openscience_aov, n=nrow(model.frame(h1)))`.

```{r, results='asis', message=FALSE}
library(stargazer)
stargazer(h4_ability, h4_openscience, type="html")
```

And finally, we predicted that our success in designing an appropriate pathway for each learner would impact how much they gained from DART (Hypothesis 5). 
We asked a number of questions probing how well their assigned pathway of models met their needs and expectations and used the mean of their responses on those items to create an index of pathway fit. 
As predicted, pathway fit significantly predicts change in ability, `r print_anova(h5_ability_aov, n=nrow(model.frame(h1)))`.
Pathway fit does not significantly predict change in open science values, though, `r print_anova(h5_openscience_aov, n=nrow(model.frame(h1)))`.

Taken together, this pattern of results is consistent with the conclusion that DART participants improved in their data science skills as a result of their participation. 
Although we did note also note an improvement in open science values from pre to post, that change appears to be independent of participants' experience with DART itself. 
This may be the result of an overall increasing commitment to open science in the pool of participants we recruited from in response to changing expectations in their fields, their own growth as researchers, etc. 

Of note is the fact that while all participants are assessed on the same set of data science skills and open science values, the actual content of individual learners' pathways differed, and for any given participant many of the measured skills would not have been explicitly covered in their modules. 
Although more targeted measurement (i.e. assessing just those skills we have explicitly taught) may have resulted in more dramatic gains, a big part of our pedagogical design for DART was around building meta-cognitive and psycho-social skills, such as resilience to failure and reduced impostor syndrome. 
It is our hope that DART participants come away with an improved *ability to learn* new data science skills, including on topics we don't currently cover -- an ability that should serve them well throughout their career in a rapidly evolving field. 
Seeing our participants report dramatic gains in self-reported data science skills across such a wide range of topics suggests that DART might be working. 
